{"cells":[{"cell_type":"markdown","metadata":{"id":"GiWulKAdmAHX"},"source":["# Transformer"]},{"cell_type":"markdown","metadata":{"id":"9KsBGZpKkWki"},"source":["Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n","1. Multi-head attention 및 self-attention 구현.\n","2. 각 과정에서 일어나는 연산과 input/output 형태 이해."]},{"cell_type":"markdown","metadata":{"id":"8qRU5DFY2OM8"},"source":["### 필요 패키지 import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDtMioSQQ1bB"},"outputs":[],"source":["from torch import nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","\n","import torch\n","import math"]},{"cell_type":"markdown","metadata":{"id":"HH0VdC4uJJVG"},"source":["## Req. 2-1 Multi-head self-attention 구조 익히기"]},{"cell_type":"markdown","metadata":{"id":"QBiZObgRep_Q"},"source":["### **데이터 전처리**\n","vocab_size 100인 가상의 시퀀스 데이터를 생성합니다. \n","\n","각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9ULZIqTenSc"},"outputs":[],"source":["pad_id = 0\n","vocab_size = 100\n","\n","data = [\n","  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n","  [60, 96, 51, 32, 90],\n","  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n","  [75, 51],\n","  [66, 88, 98, 47],\n","  [21, 39, 10, 64, 21],\n","  [98],\n","  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n","  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n","  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Hx3mcivgMyH"},"outputs":[],"source":["# 길이 맞춰주기 위해 패딩합니다.\n","def padding(data):\n","  max_len = len(max(data, key=len))\n","  print(f\"Maximum sequence length: {max_len}\")\n","\n","  for i, seq in enumerate(tqdm(data)):\n","    if len(seq) < max_len:\n","      data[i] = seq + [pad_id] * (max_len - len(seq))\n","\n","  return data, max_len"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3e8FiNvgX60","outputId":"4467b1d0-9e2e-4994-d6db-b9f3c10b20e5"},"outputs":[],"source":["data, max_len = padding(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwPSIWYugaN0"},"outputs":[],"source":["data"]},{"cell_type":"markdown","metadata":{"id":"rwqjACx8iidc"},"source":["### Hyperparameter 세팅 및 embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p-Ngp2nWimS8"},"outputs":[],"source":["d_model = 512  # model의 hidden size\n","num_heads = 8  # head의 개수\n","\n","# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJMi2Xsni5uq"},"outputs":[],"source":["embedding = nn.Embedding(vocab_size, d_model)\n","\n","# B: batch size, L: maximum sequence length\n","batch = torch.LongTensor(data)  # (B, L)\n","batch_emb = embedding(batch)  # (B, L, d_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tLCUQwojcUb"},"outputs":[],"source":["print(batch_emb)\n","print(batch_emb.shape)"]},{"cell_type":"markdown","metadata":{"id":"s0Lhx892gmi3"},"source":["### Linear projection & 여러 head로 나누기"]},{"cell_type":"markdown","metadata":{"id":"urXMBRnRgqvw"},"source":["Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DWKDqgCgfMk"},"outputs":[],"source":["w_q = nn.Linear(d_model, d_model)\n","w_k = nn.Linear(d_model, d_model)\n","w_v = nn.Linear(d_model, d_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcLuhda7m-Lm"},"outputs":[],"source":["w_0 = nn.Linear(d_model, d_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H-vSL7PwnV6k","outputId":"1707f866-ed4f-4b87-89b8-651fe4d29ac7"},"outputs":[],"source":["q = w_q(batch_emb)  # (B, L, d_model)\n","k = w_k(batch_emb)  # (B, L, d_model)\n","v = w_v(batch_emb)  # (B, L, d_model)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"]},{"cell_type":"markdown","metadata":{"id":"Wnvlum-LnF1T"},"source":["Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."]},{"cell_type":"markdown","metadata":{"id":"sXcYLZYvJT_1"},"source":["- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n","- 구현에서는 Wq, Wk, Wv 한 개씩\n","- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_tiOKAv9nEli","outputId":"f22f50b5-2830-48fe-8308-ce0ac49f65dc"},"outputs":[],"source":["batch_size = q.shape[0]\n","d_k = d_model // num_heads\n","\n","# num_heads * d_k로 쪼갠다\n","q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5tNb2isfn5Cx","outputId":"12751a52-14d7-405d-bf6f-a894e972dd10"},"outputs":[],"source":["# num_heads를 밖으로 뺌으로써\n","# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n","\n","q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"]},{"cell_type":"markdown","metadata":{"id":"NWrDA5_Sofad"},"source":["### Scaled dot-product self-attention 구현"]},{"cell_type":"markdown","metadata":{"id":"w52C4k3Wfl8m"},"source":["각 head에서 실행되는 self-attetion 과정입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5waKr0Hfi2K"},"outputs":[],"source":["# shape - (L, L)\n","# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n","attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n","# softmax - row-wise이기 때문에 dim은 -1\n","attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n","\n","print(attn_dists)\n","print(attn_dists.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7megouWpgCck","outputId":"f8d3f539-fb79-4cde-abb1-6476b86cf81f"},"outputs":[],"source":["attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n","\n","print(attn_values.shape)"]},{"cell_type":"markdown","metadata":{"id":"LmSTaymdg-P_"},"source":["### 각 head의 결과물 병합"]},{"cell_type":"markdown","metadata":{"id":"YSdQZCk0hCNd"},"source":["각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaK0bpMGhQZ2","outputId":"45f62395-8ff4-4f0b-b407-f484f2718361"},"outputs":[],"source":["attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n","attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n","\n","print(attn_values.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTng_2SXhdH1"},"outputs":[],"source":["# w_0 : (d_model, d_model)\n","# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n","outputs = w_0(attn_values)\n","\n","print(outputs)\n","print(outputs.shape)"]},{"cell_type":"markdown","metadata":{"id":"goX70VKqhxQH"},"source":["## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"]},{"cell_type":"markdown","metadata":{"id":"WtNyV7mMj7V_"},"source":["위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n","\n","아래 코드의 TODO 부분을 채워주세요."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_kNhOTrkBHm"},"outputs":[],"source":["class MultiheadAttention(nn.Module):\n","  def __init__(self):\n","    super(MultiheadAttention, self).__init__()\n","\n","    # Q, K, V learnable matrices\n","    self.w_q = nn.Linear(d_model, d_model)\n","    self.w_k = nn.Linear(d_model, d_model)\n","    self.w_v = nn.Linear(d_model, d_model)\n","\n","    # Linear projection for concatenated outputs\n","    self.w_0 = nn.Linear(d_model, d_model)\n","\n","  # scaled-dot product attention\n","  def self_attention(self, q, k, v):\n","    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n","    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n","\n","    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n","\n","    return attn_values\n","\n","  def forward(self, q, k, v):\n","    batch_size = q.shape[0]\n","\n","    # linear projection\n","    ################################################################################\n","    # TODO 1: Implement the forward pass for linear projection.                #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    # head만큼 쪼개준다\n","    ################################################################################\n","    # TODO 2: Implement the forward pass for \bsplit head.                #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n","    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","\n","    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n","    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n","\n","    return self.w_0(attn_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYLuu_9alQxT"},"outputs":[],"source":["multihead_attn = MultiheadAttention()\n","\n","outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMiXlYjSlTfB"},"outputs":[],"source":["print(outputs)\n","print(outputs.shape)  # (batch_size, length, d_model)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ptj1_Transformer_문제","provenance":[{"file_id":"1XNOcAyN3Q9KN6-9oA1t6Ueh37tJYQFGa","timestamp":1658069359050}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.5"},"vscode":{"interpreter":{"hash":"afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"}}},"nbformat":4,"nbformat_minor":0}
